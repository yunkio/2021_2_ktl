{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3261141-7b13-4fe9-93f0-cf24267f2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09531561-6ac4-452c-b74f-424c4f2d3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ecfb95a-dead-4fc9-8794-0ad1b265cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./data/data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad57cd99-158a-4030-81b3-abd77e0360c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, product_num, product_size = False):\n",
    "    data = data[data['제품종류'] == 'Product_' + str(product_num)]\n",
    "    \n",
    "    if product_size:\n",
    "        data = data[data['제품Size'] == product_size]\n",
    "    \n",
    "    y = data['선재사상압연모터전류'].values\n",
    "    data['압연시간'] = (data['압연완료일시'] - data['압연시작일시']).dt.seconds\n",
    "    data.drop(['CoilNO', '선재사상압연모터전류', '압연완료일시', '압연시작일시', '제품종류'], inplace=True, axis=1)\n",
    "    \n",
    "    if product_size:\n",
    "        data.drop(['제품Size'], inplace=True, axis=1)\n",
    "\n",
    "    x = data.values\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "    scaler = []\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     train_x = pd.DataFrame(scaler.fit_transform(train_x), columns=data.columns)\n",
    "#     test_x = pd.DataFrame(scaler.transform(test_x), columns=data.columns)\n",
    "    train_x = pd.DataFrame(train_x, columns=data.columns)\n",
    "    test_x = pd.DataFrame(test_x, columns=data.columns)\n",
    "    \n",
    "#     return train_x, test_x, train_y, test_y, scaler\n",
    "    return train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f52ef8-d3e8-4f4c-bd1b-0dc6b754e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_17, test_x_17, train_y_17, test_y_17, scaler_17 = preprocess(data, 17)\n",
    "# train_x_9, test_x_9, train_y_9, test_y_9, scaler_9 = preprocess(data, 9)\n",
    "\n",
    "train_x_17, test_x_17, train_y_17, test_y_17 = preprocess(data, 17)\n",
    "train_x_17_55, test_x_17_55, train_y_17_55, test_y_17_55 = preprocess(data, 17, 5.5)\n",
    "train_x_9, test_x_9, train_y_9, test_y_9 = preprocess(data, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a788e4-95a9-4373-8270-4ae7853874c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb dataset 만들기\n",
    "categorical_features = [] # 필요시 제품Size 추가\n",
    "training_rounds = 10000\n",
    "\n",
    "train_ds_17 = lgb.Dataset(train_x_17, label = train_y_17, categorical_feature = categorical_features) \n",
    "test_ds_17 = lgb.Dataset(test_x_17, label = test_y_17, categorical_feature = categorical_features) \n",
    "\n",
    "train_ds_17_55 = lgb.Dataset(train_x_17_55, label = train_y_17_55, categorical_feature = categorical_features) \n",
    "test_ds_17_55 = lgb.Dataset(test_x_17_55, label = test_y_17_55, categorical_feature = categorical_features) \n",
    "\n",
    "train_ds_9 = lgb.Dataset(train_x_9, label = train_y_9, categorical_feature = categorical_features) \n",
    "test_ds_9 = lgb.Dataset(test_x_9, label = test_y_9, categorical_feature = categorical_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b5524c-4426-413a-a4ce-50eeaed0e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-05 12:50:46,336]\u001b[0m A new study created in memory with name: no-name-c349d69c-a38a-4782-8804-69da240ee0e6\u001b[0m\n",
      "feature_fraction, val_score: 144.812422: 100%|###########################################| 7/7 [06:49<00:00, 58.48s/it]\n",
      "num_leaves, val_score: 138.359880: 100%|##############################################| 20/20 [36:22<00:00, 109.12s/it]\n",
      "bagging, val_score: 138.359880: 100%|#################################################| 10/10 [18:12<00:00, 109.30s/it]\n",
      "feature_fraction_stage2, val_score: 138.311914: 100%|###################################| 6/6 [12:05<00:00, 120.98s/it]\n",
      "regularization_factors, val_score: 138.246293: 100%|##################################| 20/20 [44:18<00:00, 132.90s/it]\n",
      "min_data_in_leaf, val_score: 138.246293: 100%|##########################################| 5/5 [11:35<00:00, 139.18s/it]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "params = {\n",
    "        \"objective\": 'regression',\n",
    "        \"metric\": 'mse',\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",                \n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "study_tuner = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Suppress information only outputs - otherwise optuna is \n",
    "# quite verbose, which can be nice, but takes up a lot of space\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# Run optuna LightGBMTunerCV tuning of LightGBM with cross-validation\n",
    "tuner = lgb.LightGBMTunerCV(params, \n",
    "                            train_ds_17, \n",
    "                            categorical_feature=categorical_features,\n",
    "                            study=study_tuner,\n",
    "                            verbose_eval=False,                            \n",
    "                            early_stopping_rounds=250,\n",
    "                            time_budget=19800, # Time budget of 5 hours, we will not really need it\n",
    "                            seed = 42,\n",
    "                            folds=kf,\n",
    "                            num_boost_round=10000,\n",
    "                            callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ] #[0.1]*5 + [0.05]*15 + [0.01]*45 + \n",
    "                           )\n",
    "\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67db5d6-9d6e-4ba3-9c7e-6f7c512190aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'regression', 'metric': 'l2', 'verbosity': -1, 'boosting_type': 'gbdt', 'seed': 42, 'feature_pre_filter': False, 'lambda_l1': 2.8696095424127307e-05, 'lambda_l2': 1.0443431377721304e-06, 'num_leaves': 126, 'feature_fraction': 0.552, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}\n",
      "138.24629319224178\n"
     ]
    }
   ],
   "source": [
    "print(tuner.best_params)\n",
    "# Classification error\n",
    "print(tuner.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6773147-695b-4cf6-8715-4e019ea1dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_best_params = tuner.best_params\n",
    "if tmp_best_params['feature_fraction']==1:\n",
    "    tmp_best_params['feature_fraction']=1.0-1e-9\n",
    "if tmp_best_params['feature_fraction']==0:\n",
    "    tmp_best_params['feature_fraction']=1e-9\n",
    "if tmp_best_params['bagging_fraction']==1:\n",
    "    tmp_best_params['bagging_fraction']=1.0-1e-9\n",
    "if tmp_best_params['bagging_fraction']==0:\n",
    "    tmp_best_params['bagging_fraction']=1e-9  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c696f85-5e63-4819-b86e-32d4f0b431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_best_params = {'objective': 'regression', 'metric': 'l2', 'verbosity': -1, 'boosting_type': 'gbdt', 'seed': 42, 'feature_pre_filter': False, 'lambda_l1': 2.8696095424127307e-05, 'lambda_l2': 1.0443431377721304e-06, 'num_leaves': 126, 'feature_fraction': 0.552, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}\n",
    "\n",
    "if tmp_best_params['feature_fraction']==1:\n",
    "    tmp_best_params['feature_fraction']=1.0-1e-9\n",
    "if tmp_best_params['feature_fraction']==0:\n",
    "    tmp_best_params['feature_fraction']=1e-9\n",
    "if tmp_best_params['bagging_fraction']==1:\n",
    "    tmp_best_params['bagging_fraction']=1.0-1e-9\n",
    "if tmp_best_params['bagging_fraction']==0:\n",
    "    tmp_best_params['bagging_fraction']=1e-9  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca281419-db2b-4528-9784-a8959f9a192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# We will track how many training rounds we needed for our best score.\n",
    "# We will use that number of rounds later.\n",
    "best_score = 999\n",
    "training_rounds = 10000\n",
    "\n",
    "# Declare how we evaluate how good a set of hyperparameters are, i.e.\n",
    "# declare an objective function.\n",
    "def objective(trial):\n",
    "    # Specify a search space using distributions across plausible values of hyperparameters.\n",
    "    param = {\n",
    "        \"objective\": 'regression',\n",
    "        \"metric\": 'mse',\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",                \n",
    "        \"seed\": 42,\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'seed': 1979,\n",
    "        'num_threads': multiprocessing.cpu_count(),\n",
    "         'min_data_in_leaf' : 20,\n",
    "#         'device': 'gpu',\n",
    "#         'gpu_platform_id': 0,\n",
    "#         'gpu_device_id': 0\n",
    "    }\n",
    "\n",
    "    # Run LightGBM for the hyperparameter values\n",
    "    lgbcv = lgb.cv(param,\n",
    "                   train_ds_17,\n",
    "                   categorical_feature=categorical_features,\n",
    "                   folds=kf,\n",
    "                   verbose_eval=False,                   \n",
    "                   early_stopping_rounds=250,                   \n",
    "                   num_boost_round=10000,                    \n",
    "                   callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ]\n",
    "                  )\n",
    "    cv_score = lgbcv['l2-mean'][-1] + lgbcv['l2-stdv'][-1]\n",
    "    if cv_score<best_score:\n",
    "        training_rounds = len( list(lgbcv.values())[0] )\n",
    "    \n",
    "    # Return metric of interest\n",
    "    return cv_score\n",
    "\n",
    "# Suppress information only outputs - otherwise optuna is \n",
    "# quite verbose, which can be nice, but takes up a lot of space\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# We search for another 4 hours (3600 s are an hours, so timeout=14400).\n",
    "# We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n",
    "# by optuna or set neither timeout or n_trials so that we keep going until \n",
    "# the user interrupts (\"Cancel run\").\n",
    "study = optuna.create_study(direction='minimize')  \n",
    "study.enqueue_trial(tmp_best_params)\n",
    "study.optimize(objective, timeout= 60 * 60 * 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ca5d7-2049-4370-ae4e-301c8495b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30368b67-a82b-4867-aa2a-085487868585",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d8b25-7577-46da-be43-803134616b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38908e9c-6d94-4b33-8f01-1deb93696a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf428a-ffbe-465e-b5d7-61d3a00d3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification error\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bef3a-fee4-4375-b5a3-f2169c09ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"objective\": 'regression',\n",
    "    \"metric\": 'mse',\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"seed\": 42} \n",
    "best_params.update(study.best_params)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b748f-f8ac-4a27-a3c9-c0d44365a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbfit = lgb.train(best_params,\n",
    "                   train_ds_17,\n",
    "                   categorical_feature=categorical_features,\n",
    "                   verbose_eval=False,                   \n",
    "                   num_boost_round=training_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32451ff-2abb-4a32-8ef8-27c720425a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = lgbfit.predict(train_x_17)\n",
    "predict_test = lgbfit.predict(test_x_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb95ac9-1038-4fc5-822d-06b70cc16aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_y_17, predict_test)\n",
    "r2 = r2_score(test_y_17, predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de896f-66e1-4743-9a23-49dd6f94883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean squared error: ', mse)\n",
    "print('R2 score: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12403e59-39b0-48ea-9d4f-fc3b7f78f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.concat([pd.DataFrame(test_y_17), pd.DataFrame(predict_test)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5375748-1db1-4079-8969-1187140c206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.columns = ['label','predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167c9f8-6558-4ae9-bb04-d476ef672107",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'label', y = 'predict', data = final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
